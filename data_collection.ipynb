{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50bc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== File Paths ==========\n",
    "input_file = './input/datascientists.xls'\n",
    "scientists_output_file = './output/scientists_cleaned.csv'\n",
    "papers_output_file = 'papers.csv'\n",
    "error_log_file = 'error_log.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a351e",
   "metadata": {},
   "source": [
    "========== Step 1: Load or Generate Cleaned Scientist Data =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1df89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if os.path.exists(scientists_output_file):\n",
    "    print(f\"Found existing '{scientists_output_file}', loading it...\")\n",
    "    scientists_df = pd.read_csv(scientists_output_file)\n",
    "else:\n",
    "    print(\"'scientists_cleaned.csv' not found. Starting from raw input...\")\n",
    "\n",
    "    initial_df = pd.read_excel(input_file)\n",
    "    print(\"Collecting final DBLP URLs and PIDs...\")\n",
    "    pids = []\n",
    "    final_urls = []\n",
    "    errors_links = []\n",
    "\n",
    "    for link in tqdm(initial_df['dblp']):\n",
    "        retries = 5\n",
    "        delay = 2\n",
    "        response = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(link, timeout=10)\n",
    "                if response.status_code == 429:\n",
    "                    print(\"Too many requests. Sleeping for 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                elif response.status_code == 410:\n",
    "                    print(f\"{link} is gone (410). Skipping.\")\n",
    "                    response = None\n",
    "                    break\n",
    "                elif response.status_code != 200:\n",
    "                    raise Exception(f\"HTTP Error {response.status_code}\")\n",
    "                break  # success\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1}/{retries} failed for {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(delay)\n",
    "                    delay *= 2\n",
    "                else:\n",
    "                    print(f\"Skipping {link} after {retries} failed attempts.\")\n",
    "                    response = None\n",
    "\n",
    "        if response is None:\n",
    "            pids.append('Error')\n",
    "            final_urls.append('Error')\n",
    "            errors_links.append(link)\n",
    "            continue\n",
    "\n",
    "        final_url = response.url\n",
    "        match = re.search(r'pid/(.*).html', final_url)\n",
    "\n",
    "        if match:\n",
    "            pid = match.group(1).replace('/', '-')\n",
    "            pids.append(pid)\n",
    "            final_urls.append(final_url)\n",
    "        else:\n",
    "            pids.append('Error')\n",
    "            final_urls.append('Error')\n",
    "            errors_links.append(link)\n",
    "\n",
    "    # Save cleaned data\n",
    "    cleaned_df = initial_df.copy()\n",
    "    cleaned_df['pid'] = pids\n",
    "    cleaned_df['final_url'] = final_urls\n",
    "    cleaned_df = cleaned_df[(cleaned_df['pid'] != 'Error') & (cleaned_df['final_url'] != 'Error')]\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset='pid', keep='first')\n",
    "    cleaned_df = cleaned_df.drop_duplicates()\n",
    "    cleaned_df.to_csv(scientists_output_file, index=False)\n",
    "    print(f\"Saved {len(cleaned_df)} cleaned scientists to {scientists_output_file}\")\n",
    "    scientists_df = cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ba631",
   "metadata": {},
   "source": [
    "========== Step 2: Scrape Papers (Parallelized) =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_scientist(row):\n",
    "    papers = []\n",
    "    pid = row['pid']\n",
    "    url = row['final_url']\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        entries = soup.find_all('li', class_=lambda x: x and x.startswith('entry'))\n",
    "\n",
    "        for entry in entries:\n",
    "            title_tag = entry.find('span', class_='title')\n",
    "            year_tag = entry.find('span', class_='year')\n",
    "            doi_tag = entry.find('a', title='DOI')\n",
    "            author_tags = entry.find_all('span', itemprop='author')\n",
    "\n",
    "            title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "\n",
    "            # Extract year (fallback to regex)\n",
    "            if year_tag:\n",
    "                year = year_tag.text.strip()\n",
    "            else:\n",
    "                year_matches = re.findall(r'\\b(19\\d{2}|20\\d{2})\\b', entry.text)\n",
    "                valid_years = [int(y) for y in year_matches if int(y) <= 2025]\n",
    "                if not valid_years:\n",
    "                    with open(\"missing_year_fallback.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
    "                        log_file.write(f\"No valid year for entry in {pid}:\\n{entry.text[:300]}\\n\\n\")\n",
    "                year = str(max(valid_years)) if valid_years else 'N/A'\n",
    "\n",
    "\n",
    "            doi_tag = entry.find('a', href=re.compile(r'(doi\\.org|arxiv\\.org)'))\n",
    "            doi = doi_tag['href'].strip() if doi_tag else 'N/A'\n",
    "\n",
    "            authors = ', '.join([a.text.strip() for a in author_tags]) if author_tags else 'N/A'\n",
    "\n",
    "            papers.append({\n",
    "                'Title': title,\n",
    "                'Year': year,\n",
    "                'DOI': doi,\n",
    "                'Authors': authors,\n",
    "                'file': f\"{pid}.xml\"\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(error_log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"Error for {pid} at {url}: {e}\\n\")\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88036410",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Scraping publications with multiprocessing...\")\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        all_papers_nested = list(tqdm(pool.imap(scrape_scientist, [row for _, row in scientists_df.iterrows()]), total=len(scientists_df)))\n",
    "\n",
    "    # Flatten the nested list of lists\n",
    "    all_papers = [paper for sublist in all_papers_nested if sublist for paper in sublist]\n",
    "\n",
    "    # ========== Step 3: Save Results ==========\n",
    "    papers_df = pd.DataFrame(all_papers, columns=['Title', 'Year', 'DOI', 'Authors', 'file'])\n",
    "    papers_df.to_csv(papers_output_file, index=False)\n",
    "    print(f\"Saved {len(papers_df)} papers to {papers_output_file}\")\n",
    "    print(f\"Errors (if any) logged in {error_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
